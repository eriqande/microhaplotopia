---
title: "microhap-processing-emc-edits"
author: Ellen M. Campbell
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{microhap-processing-emc-edits}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

This is a quick run-down on how you might use microhaplotopia with my (Ellen's) edits
to process microhaplot data.  I'm going to use the Chinook data that Neil has loaded 
into the original microhaplotopia and have left Neil's original vingette so you can
see how it differs, but have removed the R denotations around the code chunks so
R doesn't complain about the functions I've re-written.

I'm going to be doing some example data manipulation to show you what you can do
with the outputs from the edited microhaplotopia functions so I'm going to load 
the tidyverse with microhaplotopia.

```{r setup}
library(tidyverse)
#To install emc-edits branch of microhaplotopia, uncomment the following two lines
#library(devtools)
#devtools::install_github("eriqande/microhaplotopia", ref = "emc-edits", build_vignettes = TRUE)
library(microhaplotopia)
```

## Reading in the unfiltered_observed data from Microhaplot

I've left Neil's original read_unfiltered_observed function mostly unchanged.

Note that you can read in an entire folder or data, a selection of files within a
folder, or a single file with this function.  This function is made to read in your
unfiltered csv outputs from microhaplotopia.

```{r}
hap_raw <- read_unfiltered_observed(
  datapath = system.file(
    "extdata",
    package = "microhaplotopia"
  )
)



hap_raw_file <- read_unfiltered_observed(
  system.file(
    "extdata/gtseq65_observed_unfiltered_haplotype.csv.gz",
    package = "microhaplotopia"
  )
)
```

Now there is also a function for reading in .rds files!  This works similarly to 
read_unfiltred_observed--you can feed read_haplot_rds either the haplot rds file
for the run you're interested in (just make sure you're not giving it the 
pos_info.rds file from your haplot run) or you can toss it a whole bin of haplot 
rds files and it will parse through on it's own and pull out just the rds files 
it needs and ignore the pos_info and annotate rds files that microhaplot also 
generates.

So you'd run something like:
```
hap_raw <- read_haplot_rds("microhaplot_bin")
```
Or this, if you're interested in a single run:
```
hap_raw <- read_haplot_rds("my_haplot_run.rds")
```
(Apologies--no example rds files loaded into this package right now...).

Note that neither of these load in the "Reported haplotype" table from microhaplot.
This is intentional. With new haplot, the flags for questionable genotypes (genotypes
with more than your maximum number of alleles) are not included, so you may include
contaminated samples unless you're VERY careful.  As it's very easy to detect 
questionable genotypes in R, I suggest filtering your data independently either
with this package or on your own rather than using the reported haplotype files.

Let's take a quick look at our raw data before we move on:
```{r}
head(hap_raw)
```

Note that this is standard output from 'microhaplot'--if you read in your rds files
rather than the unfiltered outputs, you will get the same style table (the additional
columns of cigar strings and other additional information are dropped by read_haplot_rds()).

Also note that there is an extra column--"source".  This notes your file name/location.
It is not used for filtering/processing in any of the microhaplotopia functions, but
can still prove very useful for the user to confirm you've read in all the files 
you need, checking if any of your files are corrupted or truncated, etc. etc.

## Filter raw haplotype data

Neil's original microhaplotopia package bundled all of the filtering steps into 
one handy function: filter_raw_microhap_data().  I have split that function into
three distinct parts: filter_raw_microhap_data(), find_NXAlleles(), and 
find_contaminated_samples().  Let's walk through what each one does and how you 
might use them.

### Light filters

First, let's filter our data with filter_raw_microhap_data().  

Note that this requires four inputs: your raw data, the minimum haplotype depth, 
the minimum total depth, and the minimum allele balance.  

**Minimum haplotype depth** is the minimum number of reads a haplotype in an individual
at a locus can have for you to believe it's real.  You can sometimes get bleedthrough
from a neighboring cluster in your sequences, so say I had an individual at a locus 
that has 100 reads of allele1, 80 reads of allele2, and 2 reads of allele3--I'd suspect
allele3 to just be bleedthrough signal and would drop it before proceeding.  The minimum
haplotype depth is the minimum depth at which you'd expect to see a real genotype.  So
if I set this to 5, all alleles observed in an individual at a locus with fewer than 5
reads will be dropped.

**Total depth** is the minimum number of reads total you expect to observe in an individual
at a locus for you to be confident in that individual's genotype.  For example, if I 
have a fish with 700 reads of an allele and a second fish with 7 reads that same allele,
I'd obviously be way more confident in my genotype for the first fish than the second 
fish.  It's possible the second fish could be bleedthrough (see minmum haplotype depth 
notes above), or had such low signal that I missed an allele and that fish is actually a 
heterozygote.  If a fish has two or more alleles observed, the read depths are summed 
across all alleles to calculate total read depth.  For example, a fish with 20 reads for
allele1 has the same read depth as a fish with 12 reads for allele1 and 8 reads for allele2
and they have the same read depth as a fish with 8 reads for allele1, 5 reads for allele2, 
and 7 reads for allele3.

**Allele balance** is the ratio of reads for an individual haplotype in an individual at a 
locus to the reads of the most common haplotype for that individual at that locus.  For
example, if I have a fish with 200 reads of allele1 and 150 reads of allele2 at a locus, 
the allele balance of allele1 for that fish at that locus is 200/200 = 1; and the 
allele balance of allele2 for that fish at that locus is 150/200 = 0.75.  We'd expect
in a perfect word that true heterozygote genotypes would appear with equal numbers of
reads for both alleles (i.e. an allele balance of 1), but since sequencing is far from 
perfect, we will often set this parameter WAY lower than 1.

Now that we understand what each parameter is, let's filter our data:
```{r}
hap_light_filter <- filter_raw_microhap_data(
  hap_raw,
  haplotype_depth = 5,
  total_depth = 20, 
  allele_balance = 0.3
)

head(hap_light_filter)
```

Note how there's now an additional two columns in our genotype data: "TotalDepth"
and "NAlleles".  Total depth we discussed above and NAlleles is the total number
of alleles observed in that individual at that locus post-filtering.  

Quick note here: this function (as well as find_NXAlleles() and find_contaminated_samples())
does not assume your indiv.ID is unique.  It assumes that the combination of group
and indiv.ID is uniuqe.  This allows for an individual to be rerun as part of multiple
groups (i.e. runs if you're using snakemake), and still be filtered/processed as
independent samples.  We have a set of functions later for dealing with situations with
duplicate indiv.IDs.

### Detecting and handling NX alleles

NEXT!  Let's tackle NX alleles.  

Sometimes you run haplot, you get to the end and you find some haplotypes with not
just A, T, G, C in the sequence, but also N and X.  Ns indicate bases that could not
be called confidently.  If these appear at either end of your haplotype, it's likely
your sample was degraded and the sequencer just couldn't get a real read for those end
bases.  If it's in the middle, we've found that to be an indicator of flashing/processing
issues.  Xs on the other hand, are very real mutations.  They denote that your target
base in your haplotype was deleted in that individual.  However, you cannot tell from
this X how long that deletion was, so there's no way to determine if X in one fish
is the same deletion as an X in another fish.  While Xs occur naturally on occasion, 
tons of Xs in a single sample's genotypes across many loci is often a good indicator 
of a mis-ID'd species.

This handy function pulls out just the haplotype rows that have Ns and Xs in them.
Typically you'd use this on your lighly filtered data, as bleedthrough noise can
also be full of Ns and Xs but are not real signals of what's going on in your target
sample.
```{r}
NXAlleles <- find_NXAlleles(hap_light_filter)
```
Note that you could pull out Ns and Xs individually with this function like so:
```{r}
(NAlleles <- find_NXAlleles(hap_light_filter, NX = "N"))

(XAlleles <- find_NXAlleles(hap_light_filter, NX = "X"))
```

You can use this table to look for patterns in individuals, loci, and positions
of Ns and Xs in haplotypes.  Once I'm happy there aren't any weird patterns that 
suggest severely degraded samples or mis-ID'd fish, I typically will just drop 
the whole genotype for an individual at a locus where one of the alleles contains 
an N or an X.  You may choose to try to truncate your haplotypes if you samples 
look degraded to preserve the data that you do have, or you may drop a whole individual
from your dataset that looks like it may have been the wrong species.  What you 
do with this NXAllele table is up to you.  Note that this table is JUST the N and X
containing haplotypes--there may be other alleles obeserved in these fish at these
loci that do not contain Ns or Xs.

Here's how I'd drop the whole genotype for an individual at a locus where one of
the alleles contains and N or X.
```{r}
hap_NX_filter <- NXAlleles %>%
  select(group, indiv.ID, locus) %>%
  distinct() %>%
  anti_join(hap_light_filter, .)
```

### Detecting and handling genotypes with extra alleles

NEXT!  Let's look for contamination (i.e. individuals with more than a maximum number
of alleles at a locus).  Since Chinook are diploid, the maximum alleles we'd expect
to observe at a locus would be 2 (which is the default for this function).  If you
expect more than 2 or less than 2 alleles, you can set that with the max_alleles 
parameter in this function.

```{r}
BadFishGenos <- find_contaminated_samples(hap_NX_filter)

head(BadFishGenos)
```

Hey!!  This is a pretty good run, so we don't have any genotypes with extra alleles.

As an example, let's look at our raw data, which likely has TONS of extra alleles:
```{r}
exampleBadData <- find_contaminated_samples(hap_raw)

head(exampleBadData)
```
And yep, we've got some extra alleles there.  Note how this will export the whole
genotype for a fish at a locus if there are extra alleles so you can assess read-depth
and allele balance to see if you should adjust your filtering parameters.  

Let's move onto additional filters.

# Find missing samples

At this point you can look for which samples were filtered out completely when we
filtered our data.

This did not change from Neil's original code (however, know that this just checks
indiv.IDs, not indiv.ID + group, so if you have reruns, you may want to check this
with an anti_join, rather than this function).
```{r}
missing_samples <- find_missing_samples(hap_raw, hap_NX_filter)

head(missing_samples)
```

Neil also has a handy read_samplesheets() function, so we can use that to determine
if there were any additional samples that didn't even make it through the sequence
processing pipeline (either didn't sequence at all, or had such low quality reads 
that they didn't even make it to microhaplot).

Note that samplesheets have a header.  Sometimes we remove the empty rows from the
header for snakemake, so you may need to set n_skip lower than 21 for your target
samplesheets.  Just note that n_skip will be consistent across all our files, so
make sure they either all have empty rows, or all have the empty rows deleted.
```{r}
sampsheets <- read_samplesheets(
  system.file(
    "extdata/SampleSheets",
    package = "microhaplotopia"
  ), 
  n_skip = 21
)

#Our samples in the sample sheet are noted "CH" for chinook, but are "chinook" 
#not "CH" in haplot, so fixing that with this next code chunk (also dropping 
#empty rows)
sampsheets <- sampsheets %>%
  mutate(Sample_ID = gsub("CH_", "chinook", Sample_ID)) %>%
  filter(!is.na(Sample_ID))

#find missing samples
complete_fails <- sampsheets %>% filter(!Sample_ID %in% hap_raw$indiv.ID)

head(complete_fails)
```
And hey! No samples dropped out before running microhaplot, so that's good!

If you are using NMFS IDs as the haplot sample IDs (like what we do in our snakemake
pipeline) you can split the Sample_Plate info into NMFS ID, Box, and Well.  Here's 
how you might do that!

```{r}
sampsheets_NMFSIDs <- sampsheets %>%
  separate(Sample_Plate, into = c("NMFS_DNA_ID", "BOX", "WELL"), extra = "drop")

complete_fails_nmfs <- sampsheets_NMFSIDs %>% filter(!NMFS_DNA_ID %in% hap_raw$indiv.ID)

head(complete_fails)
```
Obviously this just spits back our entire samplesheet since none of our samples 
in the example data are named by their NMFS IDs.

# Find duplicates (i.e. reruns)

You may have some sample reruns (i.e. samples with the same sample ID, but were 
run on different miseq runs for some reason or another) OR have accidentally given
two different fish the same indiv.ID in your microhaplot run.  If this is the case, 
we have a bit of code that can identify samples with the same indiv.ID but different
groups. 

Let's make a quick fake sample dataset that has sample "chinook18699" in there 
three times: the original run; a second run with group "FakeRun1" and missing the
genotype from locus "tag_id_2_123"; and a third run with group "FakeRun2" and 
missing the genotypes from locus "tag_id_2_123" and "tag_id_2_206"
```{r}
FakeDupData <- hap_light_filter %>% 
  filter(indiv.ID == "chinook18699" & locus != "tag_id_2_123") %>%
  mutate(group = "FakeRun1") %>% 
  bind_rows(hap_light_filter)

FakeDupData <- hap_light_filter %>% 
  filter(indiv.ID == "chinook18699" & !(locus %in% c("tag_id_2_123", "tag_id_2_206"))) %>%
  mutate(group = "FakeRun2") %>% 
  bind_rows(FakeDupData)

```

## Find duplicates

We can run find_duplicates() to find duplicate samples in our fake dataset like so:
```{r}
find_duplicates(FakeDupData)
```

Now at this point we have two options: we can keep all of our runs, but give them 
unique names so they don't cause any issues moving forward, or we can just keep 
the one that genotypes at the most loci.

## Rename duplicates

For the first option, we can use the resolve_duplicate_samples() function with
the "rename" option.  This will rename each of our runs adding an "_1" to the run 
with the name that appears first when sorted by group, an "_2" to the next one, and
so on.  Let's see that in action:

```{r}
FakeDupData_DupsRename <- resolve_duplicate_samples(FakeDupData, resolve = "rename")

#Let's look at just our duplicates to demonstrate how rename works
head(filter(FakeDupData_DupsRename, grepl("chinook18699", indiv.ID)) %>%
  arrange(locus, indiv.ID))
```

## Drop duplicates

Alternatively you can use the "drop" option.  So, rather than keeping all of our
runs, we keep just the run that has the most loci genotyped.  In our fake data's 
case, we know that the original data (group "WRcarc") has the most loci passing 
ilter, so if we run this, we should only have data from the "WRcarc" group for 
fish "chinook18699"

```{r}
FakeDupData_DupsDropped <- resolve_duplicate_samples(FakeDupData, resolve = "drop")

#Let's look at our duplicate fish and confirm that the only group is now "WRcarc"
filter(FakeDupData_DupsDropped, grepl("chinook18699", indiv.ID)) %>%
  distinct(group, indiv.ID)
```

Yep!  So we can see this drops the other two runs that did not have as many loci 
genotyped.  If two runs have the same number of loci genotyped, this will keep 
whichever run appears first in your dataset.

Notice how this ONLY finds samples with the same name.  For some datasets, it is
entirely possible to find samples that have different IDs but are actually the same
individual.  We'll go through how to use Rubias to detect that type of duplicate
in the "Reformatting your data" section below.

# Remove loci

Neil has a "remove_loci" function.  Here's how you might use it to drop loci, but
you can also totally use anti_join() or filter() in the dplyr package to the same
effect if you're more comfortable with those.

```{r}
loci2chuck <- c(
  "tag_id_1079", 
  "tag_id_1227", 
  "tag_id_1629", 
  "tag_id_1692"
)

hap_DropLocs <- filter_bad_loci(
  long_genos = hap_NX_filter,
  bad_loci = loci2chuck
)

head(hap_DropLocs) 
```

# Calculate missing data per sample

We've got a handy calculate_missing_data() function.  Quick note about this function:
it counts up the total number of loci in your datasetand assumes that's the tptal 
number of loci you've run for all your samples.  Obviously this is an issue if you have
a locus that's in your pool but just didn't genotype well with the samples in your datset
OR if you ran different number of loci for different samples, so look at the outputs in 
this table carefully, and calculate your own totals if needed.

Let's do a quick example where I drop a locus ("tag_id_999") to illustrate how
that effects the total number of loci.

```{r}
hap_miss_example <- hap_NX_filter %>%
  filter(!locus == "tag_id_999")

calculate_missing_data(hap_miss_example) %>%
  head()
```

Note that n_loci is 124, when we know this panel has 125 total loci.  Note that 
there is also an n_typed column which is the total number of loci typed at that 
locus as well as an n_miss column (which is just n_loci - n_typed), so you can 
re-calculate these as needed if your n_loci column is off.

# Filter missing data

Finally, we have a function to remove samples with lots of missing data: 
filter_missing_data().

The argument 'n_locs' is the minimum number of loci a fish must be genotyped at for
you to keep it in your final dataset.

Note that this uses the "indiv.ID" column assuming it is unique so you MUST deal
with duplicate indiv.IDs BEFORE running this function (see "Find duplicates (i.e. 
reruns)" section abaove).

```{r}
hap_NoLowTypers <- filter_missing_data(hap_NX_filter, n_locs = 100)
```

# Add second allele to homozygotes

Currently your dataset has one row for homozygote samples and ideally two for your
heterozygotes at a locus.  When converting your data to other data formats, you'll 
need two rows for all of your samples.  This function adds those extra lines in
for you.  You can do this at any point in your processing after filtering your 
raw data, but I tend to do it right before I need to convert my data to other formats.

```{r}
head(arrange(hap_NoLowTypers, indiv.ID, locus))

hap_HomsFixed <- add_hom_second_allele(hap_NoLowTypers)

head(arrange(hap_HomsFixed, indiv.ID, locus))
```
You can see we now have two lines for each individual at each locus.

# Data summaries

Neil has a couple of functions to summarize your data (summarize_data(), 
plot_run_statistics()).  I highly recommend that you explore your data.  You can
use these functions to explore each field of your dataset, but I suggest also 
looking at your NAlleles, your genotypes with 3+ alleles passing filter, etc. that
may not be in your possible summary options with those functions.  I also strongly
suggest looking at logs to make sure every part of your pipeline before your haplot
run worked as expected.  I'm not going to provide code here as code typically varies 
dramatically depending on the run and what issues or interesting data you run into
when you start processing your samples.

#Reformatting your data

Your data is currently in a long formatted tidy-verse object called a "tibble." 
This makes it super convenient for exploring/summarizing your data, but most
downstream analyses require your data in a different format.  Neil has made a
function called "mhap_transform" that converts your data into some usual input 
files.  I've adjusted/added a few additional ones.  Your current options are:

  1. two-column format with haplotypes ("2col_hap"). 
  Standard two-column genotype data (two columns for each locus, one for each
  allele at that locus) where the entries are strings representing the haplotype
  alleles at that locus.
  
  1. two-column format with numerically coded haplotypes ("2col_numeric"). 
  Same information as 2col_hap, but each unique haplotype is given a unique number
  (starting at 1 and going up).  This output includes a key that tells you which 
  number corresponds to which haplotype.
  
  1. two-column format with numberically coded haplotypes without key ("2col_ndigit").
  Same as 2col_hap, but A is coded as 1, C as 2, G as 3, T as 4.  Similar to the
  2col_numeric, but no need for a key.
  
  1. CKMRsim input.  Compatible with Eric Anderson's CKMRsim package.
  
  1. rubias input.  Compatible with Eric Anderson's rubias package for GSI analysis
  (what we are currently using for matching sample analyses).
  
  1. FRANz input.  Will export a .tsv file that is equivalent to the .dat input
  needed for FRANz.  Note that this requires external metadata.
  
  1. adegenet input.  Will export a genind object compatible with adegenet. Can 
  feed it the name of a column you wish to use as populations for your samples if 
  necessary.
  
Here's a quick rundown on how you might run each of these options!

## 2 column haplotype
Here's a quick look at the 2-column haplotype format
```{r}
mhap_transform(hap_HomsFixed, "2col_hap") %>%
  head()
```
(See note after 2 column n-digit for note on missing data NAs vs Os)

## 2 column numeric
Here's a quick look at the 2-column numeric format
```{r}
two_col_numeric <- mhap_transform(hap_HomsFixed, "2col_numeric")

two_col_numeric$data %>%
  head()

two_col_numeric$key %>%
  head()
```
(See note after 2 column n-digit for note on missing data NAs vs Os)

## 2 column n-digit
Here's a quick look at the 2-column n-digit format
```{r}
mhap_transform(hap_HomsFixed, "2col_ndigit")
```
Note that missing data in all of these two-column formats is coded as NA by default.
Some programs require NAs to be coded as 0s.  I've found the easiest way to do this
is to write my two-column output to a csv, using the "na" option to code NAs as 0s,
like so:

```{r}
two_col_numeric$data %>%
  write_csv(., "two_col_numeric.csv", na = "0")
```

## rubias

Here's a quick look at rubias formatting
```{r}
haps4rubias <- mhap_transform(
  long_genos = hap_HomsFixed, 
  program = "rubias"
)

head(haps4rubias)
```
### matching samples with rubias

Rubias' matching samples function is really fast and awesome, but, being a GSI
analysis package, expects GSI data.  We can get around that by feeding it bogus
metadata.  Below is the code for how we do that:

```
library(rubias)

# Rubias expects samples as part of GSI analysis, so it wants some samples labeled as mixture and some as reference pops.  We lie to it and make this req(uired) tibble of info.  If you're actually running GSI, obviously use metadata here that makes sense instead of making this nonsense req table.
req <- tibble(sample_type = "mixture", repunit = NA, collection = "silly")
req[1, 1] = "reference"

#Add your required silly metatata
ForMatchy <- haps4rubias %>% 
  cbind(req, .)

#Adjust code below to meet your preferred matching parameters, note that:
#min_frac_non_miss = minimum fraction of loci a pair of individuals must share in order to be reported as a potential match
#min_frac_matching = minimum fraction of shared, non-missing loci that must match for samples to be reported as a match
matches <- close_matching_samples(ForMatchy, 5, min_frac_non_miss = 0.4, min_frac_matching = 0.9)
```
If you have rubias installed, you can run this and see we don't have any matching samples in our dataset.

## adegenet

Here's how you might prep an in-file for adegenet without populations:
```{r}
mhap_transform(
  long_genos = hap_HomsFixed, 
  program = "adegenet"
)
```

And let's say you want to use group as your population (although you can totally
bind on metadata and use whatever column you want--just feed it that column name
instead of "group" in the code below):
```{r}
mhap_transform(
  long_genos = hap_HomsFixed, 
  program = "adegenet",
  pops = "group"
)
```

## CKMRsim

Here's a quick look at CKMRsim formatting
```{r}
mhap_transform(
  long_genos = hap_HomsFixed, 
  program = "CKMRsim"
) %>%
  head()
```

## FRANz

Finally, here's how you might prep a FRANz file if you had a metadata object called
"meta"

For franz, your metadata must be in the following order: indiv.ID, birth_year, death_year, 
sex. Sex is M, F or ? (for unknown).  No additional columns allowed in that object.

**NOTE** I (Ellen) have not tested this function to make sure it's compatible with
my edits of the other microhaplotopia functions.  Let me know if I broke it and I'm
happy to fix it.

```
mhap_transform(
  long_genos = hap_HomsFixed, 
  program = "franz",
  metadata = meta
)
```
